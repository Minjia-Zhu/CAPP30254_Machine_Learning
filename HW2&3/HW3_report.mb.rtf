{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf820
{\fonttbl\f0\froman\fcharset0 TimesNewRomanPSMT;\f1\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;\red68\green68\blue68;\red38\green38\blue38;\red255\green255\blue255;
\red83\green83\blue83;\red0\green4\blue6;}
{\*\expandedcolortbl;;\csgenericrgb\c26667\c26667\c26667;\cssrgb\c20000\c20000\c20000;\cssrgb\c100000\c100000\c100000;
\cssrgb\c40000\c40000\c40000;\cssrgb\c0\c1176\c1961;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\ri0\sl480\slmult1\partightenfactor0

\f0\fs24 \cf2 compares the performance of the different classifiers across all the metrics for the data set used in the last assignment. Which classifier does better on which metrics? Which one runs faster? What would be your recommendation to someone who's working on the credit model?\cf0 \
\pard\pardeftab720\partightenfactor0

\f1\b\fs30 \cf3 \cb4 \expnd0\expndtw0\kerning0
Advantages of Logistic Regression:
\b0 \cf5  Lots of ways to regularize your model, and you don\'92t have to worry as much about your features being correlated, like you do in Naive Bayes. You also have a nice probabilistic interpretation, unlike decision trees or SVMs, and you can easily update your model to take in new data (using an online gradient descent method), again unlike decision trees or SVMs. Use it if you want a probabilistic framework (e.g., to easily adjust classification thresholds, to say when you\'92re unsure, or to get confidence intervals) or if you expect to receive more training data in the future that you want to be able to quickly incorporate into your model.\cb1 \

\b \cf3 \cb4 Advantages of Decision Trees:
\b0 \cf5  Easy to interpret and explain (for some people \'96 I\'92m not sure I fall into this camp). They easily handle feature interactions and they\'92re non-parametric, so you don\'92t have to worry about outliers or whether the data is linearly separable (e.g., decision trees easily take care of cases where you have class A at the low end of some feature x, class B in the mid-range of feature x, and A again at the high end). One disadvantage is that they don\'92t support online learning, so you have to rebuild your tree when new examples come on. Another disadvantage is that they easily overfit, but that\'92s where ensemble methods like random forests (or boosted trees) come in. Plus, random forests are often the winner for lots of problems in classification (usually slightly ahead of SVMs, I believe), they\'92re fast and scalable, and you don\'92t have to worry about tuning a bunch of parameters like you do with SVMs, so they seem to be quite popular these days.\cb1 \

\b \cf3 \cb4 Advantages of SVMs:
\b0 \cf5  High accuracy, nice theoretical guarantees regarding overfitting, and with an appropriate kernel they can work well even if you\'92re data isn\'92t linearly separable in the base feature space. Especially popular in text classification problems where very high-dimensional spaces are the norm. Memory-intensive, hard to interpret, and kind of annoying to run and tune, though, so I think random forests are starting to steal the crown.\cb1 \
\pard\pardeftab720\partightenfactor0

\fs56 \cf6 \cb4 But\'85\cb1 \
\pard\pardeftab720\partightenfactor0

\fs30 \cf5 \cb4 Recall, though, that better data often beats better algorithms, and designing good features goes a long way. And if you have a huge dataset, then whichever classification algorithm you use might not matter so much in terms of classification performance (so choose your algorithm based on speed or ease of use instead).\cb1 \
\cb4 And to reiterate what I said above, if you really care about accuracy, you should definitely try a bunch of different classifiers and select the best one by cross-validation. Or, to take a lesson from the Netflix Prize (and Middle Earth), just use an ensemble method to choose them all.}